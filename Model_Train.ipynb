{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":256781,"status":"error","timestamp":1635989045777,"user":{"displayName":"Po-Chen (劉柏辰) Liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641391203079784915"},"user_tz":-480},"id":"jxZMm2N2X9mX","outputId":"7dd2dbda-c280-432f-e1f8-4b5012121920"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\n","'''\n","#################### CONNECT TO THE FOLDER ####################\n","'''\n","def connect_to_the_folder(method):\n","    import os\n","    if method == 'google':\n","        from google.colab import drive\n","        drive.mount('/content/drive/', force_remount=True)\n","        os.chdir('/content/drive/My Drive/Data_Technology/DT_Learning/III_Big_Data/BDSE21_Team1/Final_Project/')\n","    elif method == 'local':\n","        os.chdir('c:\\Shared\\BDSE_Team1\\Final_Project\\\\')\n","\n","'''\n","#################### GET FIRST N POPULAR CODE ####################\n","'''\n","def get_first_n_popular_code(n):\n","    import pandas as pd\n","    import datetime\n","    end_year = datetime.datetime.now().year\n","    years = pd.Series(pd.date_range('2016', end=str(end_year + 1), freq='Y')).dt.year\n","\n","    df = []\n","\n","    for year in years:\n","        filename = f'Dataset_A/News_Anue_Arrange/News_Anue_Arrange_{year}.csv'\n","        df_read = pd.read_csv(filename, index_col=0, dtype=str)\n","        for code in df_read['code']:\n","            df.append(str(code))\n","    \n","    df = pd.DataFrame(df)\n","    df = df.groupby(df.columns.tolist(), as_index=False).size()\n","    df = df.sort_values(by='size', ascending=False)\n","    df = df.reset_index(drop=True)\n","    df = df.rename(columns={0:'code'})\n","    df = df[:n]\n","    return df\n","\n","\n","'''\n","#################### TRAINING MODEL - RNN ####################\n","'''\n","def create_training_data_all(n, y_label, start_year, day):\n","    import pandas as pd\n","    codes = get_first_n_popular_code(n)['code']\n","    df_all = pd.DataFrame([])\n","    for code in codes:\n","        filename = f'Dataset_A/Training_Data_Stock/{str(code)}_training.csv'\n","        df = pd.read_csv(filename, index_col=0)\n","        df = df[df.index > start_year]\n","        df = df.dropna()\n","        # df = df[['open', 'high', 'low', 'close', f'1days_before_{y_label}', f'2days_before_{y_label}', f'3days_before_{y_label}', f'4days_before_{y_label}', f'5days_before_{y_label}', f'6days_before_{y_label}', f'y_label_{y_label}_{day}days_after']]\n","        df = df[['open', 'high', 'low', 'close', f'1days_before_{y_label}', f'2days_before_{y_label}', f'3days_before_{y_label}', f'4days_before_{y_label}', f'5days_before_{y_label}', f'nlp_lstm', f'y_label_{y_label}_{day}days_after']]\n","        # df = df[['close', f'1days_before_{y_label}', f'2days_before_{y_label}', f'3days_before_{y_label}', f'4days_before_{y_label}', f'5days_before_{y_label}', f'6days_before_{y_label}', f'7days_before_{y_label}', f'8days_before_{y_label}', f'9days_before_{y_label}', f'y_label_{y_label}_{day}days_after']]\n","        df_all = df_all.append(df)\n","    df = df_all.sample(frac=1)\n","    return df\n","\n","def train_each_model_rnn(df, split, epochs, filename):\n","    import pandas as pd\n","    import numpy as np\n","    \n","    data_all = pd.DataFrame(df.values.flatten())\n","    data_all = np.array(data_all).astype(float)\n","\n","    # Standardization\n","    from sklearn.preprocessing import MinMaxScaler\n","    scaler = MinMaxScaler()\n","    data_all = scaler.fit_transform(data_all)\n","\n","    # train_x, train_y, test_x, test_y\n","    data = []\n","    sequence_length = len(df.columns) # Feature Number\n","    for i in range(int(len(data_all) / sequence_length)):\n","        data.append(data_all[i*sequence_length:(i+1)*sequence_length])\n","    reshaped_data = np.array(data).astype('float64')\n","    x = reshaped_data[:, :-1]\n","    y = reshaped_data[:, -1]\n","\n","    split_boundary = int(reshaped_data.shape[0] * split)\n","    train_x = x[: split_boundary]\n","    test_x = x[split_boundary:]\n","    train_y = y[: split_boundary]\n","    test_y = y[split_boundary:]\n","\n","    # Train\n","    from keras.models import Sequential\n","    from keras.layers import LSTM, Dense\n","    model = Sequential()\n","    model.add(LSTM(batch_size=None,\n","                   input_shape=(10,1),\n","                   units=256,\n","                   unroll=False))\n","    model.add(Dense(units=1))\n","    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    history = model.fit(train_x, train_y, batch_size=100, epochs=epochs, validation_split=0.1, verbose=2)\n","\n","    # Save model and detail\n","    model.save(filename + '.h5')\n","\n","    # Validate\n","    import tensorflow as tf\n","    predict = model.predict(test_x)\n","    predict = np.reshape(predict, (predict.size, ))\n","    predict = scaler.inverse_transform([[i] for i in predict])\n","    text_y = scaler.inverse_transform(test_y)\n","\n","    import matplotlib.pyplot as plt\n","    import math\n","    plt.plot(text_y, 'r-', label='real')\n","    plt.plot(predict, 'b:', label='pred')\n","    plt.legend(['predict', 'realdata'], loc='best')\n","    plt.ylim(0, math.ceil(max(np.max(predict), np.max(test_y)) * 10) / 10)\n","    plt.savefig(filename + '_pred.png')\n","    plt.show()\n","    plt.close()\n","\n","    epochs_range = range(epochs)\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    history = [acc, val_acc, loss, val_loss]\n","    history = pd.DataFrame(history)\n","    history.to_csv(filename + '.csv')\n","\n","    plt.figure(figsize=(16, 9))\n","    plt.suptitle(filename)\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs_range, acc, label='Training Accuracy')\n","    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","    plt.legend(loc='upper left')\n","    plt.title('Training and Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs_range, loss, label='Training Loss')\n","    plt.plot(epochs_range, val_loss, label='Validation Loss')\n","    plt.legend(loc='upper left')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","\n","    plt.savefig(filename + '_detail.png')\n","    plt.close()\n","\n","def train_model_rnn(n, y_labels, start_year, days, split, epochs):\n","    for y_label in y_labels:\n","        for day in days:\n","            filename = f'Model/Model_Save/rnn_5_nlp/rnn_nlp_{y_label}_{day}_{epochs}'\n","            df = create_training_data_all(n, y_label, start_year, day)\n","            train_each_model_rnn(df, split, epochs, filename)\n","            print(filename, '--- complete ---')\n","\n","if __name__ == '__main__':\n","    connect_to_the_folder('google') # connect to google or local\n","    # y_labels = ['close', 'increase']\n","    # days = [1, 2, 3, 4, 5]\n","    y_labels = ['close']\n","    days = [1, 2, 3, 4, 5]\n","    n, start_year, split, epochs = 50, '2011', 0.8, 300\n","    train_model_rnn(n, y_labels, start_year, days, split, epochs)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNfpiBn/hXjuxbGxUUldOIp","collapsed_sections":[],"name":"Model_Train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
